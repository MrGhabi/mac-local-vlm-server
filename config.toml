[server]
port = 58080
host = "127.0.0.1"

[model]
default_model = "mlx-community/Qwen3-VL-8B-Instruct-4bit"
venv_path = "~/envs/qwen3-vl"
allowed_models = [
    "mlx-community/Qwen3-VL-4B-Instruct-4bit",
    "mlx-community/Qwen3-VL-8B-Instruct-4bit",
    "mlx-community/Qwen3-VL-30B-A3B-Instruct-4bit",
    "mlx-community/Qwen3-30B-A3B-Instruct-2507-4bit",
]

[inference]
max_tokens = 256
temperature = 0.1

[paths]
pid_file = "/tmp/llm-server.pid"
log_file = "~/.llm-server.log"
